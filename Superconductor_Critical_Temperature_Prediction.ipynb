{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThfNXfUcv8jn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from copy import deepcopy\n",
        "\n",
        "# Global parameters for DFDBCSO\n",
        "PopSize = 200  # Increased population size\n",
        "MaxIter = 500  # Increased iterations\n",
        "phi = 0.1      # Increased mutation intensity\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data():\n",
        "    data = pd.read_csv(\"/content/train.csv\")  # Replace with your dataset path\n",
        "\n",
        "    # Drop rows with NaN in target or features\n",
        "    data = data.dropna(subset=[\"critical_temp\"])\n",
        "    X = data.drop(columns=[\"critical_temp\"])\n",
        "    y = data[\"critical_temp\"]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Define regression models\n",
        "def define_models():\n",
        "    models = {\n",
        "        \"RandomForest\": RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42),\n",
        "        \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=8, random_state=42),\n",
        "        \"SVR\": SVR(kernel='rbf', C=10, gamma=0.1),\n",
        "        \"Ridge\": Ridge(alpha=1.0),\n",
        "        \"Lasso\": Lasso(alpha=0.01, random_state=42),\n",
        "        \"ExtraTrees\": ExtraTreesRegressor(n_estimators=200, max_depth=15, random_state=42),\n",
        "        \"AdaBoost\": AdaBoostRegressor(n_estimators=200, random_state=42),\n",
        "        \"KNN\": KNeighborsRegressor(n_neighbors=10),\n",
        "        \"DecisionTree\": DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "    }\n",
        "    return models\n",
        "\n",
        "# Train and evaluate models\n",
        "def train_and_evaluate_models(models, X_train, X_val, y_train, y_val, X_test, y_test):\n",
        "    results = []\n",
        "    predictions = {}\n",
        "    test_results = []\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Validation set predictions\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        predictions[name] = y_val_pred\n",
        "\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "        val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "        # Test set predictions\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "        test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "        results.append([name, val_mse, val_mae, val_r2])\n",
        "        test_results.append([name, test_mse, test_mae, test_r2])\n",
        "\n",
        "        print(f\"{name} Validation Results:\\nMSE: {val_mse:.4f}, MAE: {val_mae:.4f}, R2: {val_r2:.4f}\")\n",
        "        print(f\"{name} Test Results:\\nMSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R2: {test_r2:.4f}\\n\")\n",
        "\n",
        "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Validation MSE\", \"Validation MAE\", \"Validation R2\"])\n",
        "    test_results_df = pd.DataFrame(test_results, columns=[\"Model\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
        "\n",
        "    # Sort by Test MSE to select top 3\n",
        "    test_results_df = test_results_df.sort_values(by=\"Test MSE\", ascending=True)\n",
        "\n",
        "    return results_df, test_results_df, predictions\n",
        "\n",
        "# DFDBCSO optimization for ensemble weights\n",
        "def calculate_fdb_scores(weights, predictions, y_val):\n",
        "    fitness = []\n",
        "    for w in weights:\n",
        "        ensemble_pred = sum(w[i] * predictions[name] for i, name in enumerate(predictions.keys()))\n",
        "        mse = mean_squared_error(y_val, ensemble_pred)\n",
        "        fitness.append(-mse)  # Minimize MSE only\n",
        "    return np.array(fitness)\n",
        "\n",
        "def dfdbcso_ensemble(predictions, y_val):\n",
        "    global PopSize, MaxIter, phi\n",
        "    num_models = len(predictions)\n",
        "    weights = np.random.uniform(0, 1, size=(PopSize, num_models))\n",
        "    weights /= np.sum(weights, axis=1, keepdims=True)  # Ensure weights sum to 1\n",
        "    velocity = np.zeros((PopSize, num_models))\n",
        "    best_weights = weights.copy()\n",
        "    best_fitness = np.full(PopSize, -np.inf)\n",
        "\n",
        "    for cur_iter in range(MaxIter):\n",
        "        alpha = cur_iter / MaxIter\n",
        "        fitness = calculate_fdb_scores(weights, predictions, y_val)\n",
        "\n",
        "        # Update pbest\n",
        "        for i in range(PopSize):\n",
        "            if fitness[i] > best_fitness[i]:\n",
        "                best_weights[i] = weights[i]\n",
        "                best_fitness[i] = fitness[i]\n",
        "\n",
        "        # Global best\n",
        "        gbest_idx = np.argmax(best_fitness)\n",
        "        gbest = best_weights[gbest_idx]\n",
        "\n",
        "        # Update velocity and position\n",
        "        for i in range(PopSize):\n",
        "            velocity[i] = np.random.rand(num_models) * velocity[i] + \\\n",
        "                          np.random.rand(num_models) * (best_weights[i] - weights[i]) + \\\n",
        "                          phi * (gbest - weights[i])\n",
        "            weights[i] = np.clip(weights[i] + velocity[i], 0, 1)\n",
        "            weights[i] /= np.sum(weights[i])\n",
        "\n",
        "    return gbest, -best_fitness[gbest_idx]  # Return MSE\n",
        "\n",
        "# Visualize results\n",
        "def visualize_results(results_df, test_results_df, ensemble_mse, ensemble_mae, ensemble_r2, y_test, ensemble_pred):\n",
        "    # Bar plot for model performance on validation set\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Model', y='Validation R2', data=results_df)\n",
        "    plt.title(\"Model Performance on Validation Set (R2 Score)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # Bar plot for model performance on test set with ensemble\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ensemble_row = pd.DataFrame({\n",
        "        \"Model\": [\"Ensemble (DFDBCSO)\"],\n",
        "        \"Test MSE\": [ensemble_mse],\n",
        "        \"Test MAE\": [ensemble_mae],\n",
        "        \"Test R2\": [ensemble_r2]\n",
        "    })\n",
        "    test_results_with_ensemble = pd.concat([test_results_df, ensemble_row], ignore_index=True)\n",
        "    colors = [\"gold\" if model == \"Ensemble (DFDBCSO)\" else \"blue\" for model in test_results_with_ensemble[\"Model\"]]\n",
        "    ax = sns.barplot(x='Model', y='Test R2', data=test_results_with_ensemble, palette=colors)\n",
        "    plt.title(\"Model Performance (R2 Score)\")\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Annotate bars with their values\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(format(p.get_height(), '.4f'),\n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha = 'center', va = 'center',\n",
        "                    xytext = (0, 10),\n",
        "                    textcoords = 'offset points')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Plot true vs predicted values for ensemble model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, ensemble_pred, color='green', alpha=0.6, label='Predicted vs Actual')\n",
        "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', label='Ideal Prediction')\n",
        "    plt.xlabel(\"Actual Values\")\n",
        "    plt.ylabel(\"Predicted Values\")\n",
        "    plt.title(\"DFDBCSO Ensemble Model: Actual vs Predicted\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Optimized Ensemble Model MSE: {ensemble_mse:.4f}, MAE: {ensemble_mae:.4f}, R2: {ensemble_r2:.4f}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_preprocess_data()\n",
        "\n",
        "    # Define models\n",
        "    models = define_models()\n",
        "\n",
        "    # Train and evaluate models\n",
        "    results_df, test_results_df, predictions = train_and_evaluate_models(models, X_train, X_val, y_train, y_val, X_test, y_test)\n",
        "    print(\"Validation Performance:\")\n",
        "    print(results_df)\n",
        "    print(\"Test Performance:\")\n",
        "    print(test_results_df)\n",
        "\n",
        "    # Select top 3 models based on test performance\n",
        "    top_models = test_results_df.head(3)[\"Model\"].values\n",
        "    print(\"Top 3 models:\", top_models)\n",
        "    top_predictions = {name: predictions[name] for name in top_models}\n",
        "\n",
        "    # Optimize ensemble weights using full DFDBCSO\n",
        "    best_weights, best_mse = dfdbcso_ensemble(top_predictions, y_val)\n",
        "    print(\"Optimized Ensemble Weights:\", best_weights)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    ensemble_pred = sum(best_weights[i] * models[top_models[i]].predict(X_test) for i in range(len(top_models)))\n",
        "    mse = mean_squared_error(y_test, ensemble_pred)\n",
        "    mae = mean_absolute_error(y_test, ensemble_pred)\n",
        "    r2 = r2_score(y_test, ensemble_pred)\n",
        "    print(f\"Test Set Ensemble MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(results_df, test_results_df, mse, mae, r2, y_test, ensemble_pred)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "eYmBOO_1Zr04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}